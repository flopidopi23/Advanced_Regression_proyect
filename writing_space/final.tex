% !TEX program = lualatex
\documentclass[8pt,twocolumn]{article}

% Essential packages
\usepackage{mdframed}
\usepackage{xcolor}

% Define a nice framed box for the abstract
\definecolor{lightgray}{gray}{0.95}
\newenvironment{abstractbox}{%
  \begin{mdframed}[backgroundcolor=lightgray,
                   linewidth=0.5pt,
                   linecolor=black,
                   topline=true,
                   bottomline=true,
                   leftline=true,
                   rightline=true,
                   innerleftmargin=10pt,
                   innerrightmargin=10pt,
                   innertopmargin=10pt,
                   innerbottommargin=10pt]
}{%
  \end{mdframed}
}

% Essential packages
\usepackage{fontspec}        % Font selection for LuaLaTeX
\usepackage{amsmath,amssymb} % Math symbols and environments
\usepackage{graphicx}        % For including images
\usepackage{booktabs}        % For professional tables
\usepackage{hyperref}        % For hyperlinks
\usepackage{geometry}        % For page layout
\usepackage{algorithm}       % For algorithms
\usepackage{algpseudocode}   % For algorithm pseudocode
\usepackage{natbib}    
\bibliographystyle{apalike}       % For bibliography
% Page layout
\geometry{
  paper=a4paper,
  top=0.5cm,
  bottom=0.5cm,
  left=0.5cm,
  right=0.5cm,
}

% Remove large paragraph indents
\setlength{\parindent}{0pt}   % Remove paragraph indentation completely
\setlength{\parskip}{0.1cm}   % Add some vertical space between paragraphs

\setmainfont{TeX Gyre Termes}[
  Ligatures=TeX,
  Numbers=OldStyle
]

% Section styling
\usepackage{titlesec}

% Format section headings
\titleformat{\section}
  {\normalfont\large\bfseries}  % format
  {\thesection}                 % label
  {1em}                         % sep
  {\MakeUppercase}              % before-code
  []                            % after-code

% Adjust spacing around sections
\titlespacing*{\section}
  {0pt}                         % left
  {1.5ex plus .5ex minus .2ex}  % before
  {1ex plus .2ex} 


% Font settings - using TeX Gyre Termes (Times-like font) for maximum compatibility
\usepackage{fontspec}
\setmainfont{TeX Gyre Termes}[
  Ligatures=TeX,
  Numbers=OldStyle
]

% Bibliography settings
\bibliographystyle{plainnat}

% Hyperref settings
\hypersetup{
  colorlinks=true,
  linkcolor=black,
  citecolor=black,
  urlcolor=black,
}

% Title information
\title{\textbf{Paper Title and Its Main Contribution}}
\author{First Author and Second Author\\
}
\date{}

\begin{document}

\maketitle

\begin{abstractbox}
\noindent\textbf{Abstract} \\
Your abstract goes here. Variable selection is an important topic in linear regression analysis. In practice, a large number of predictors are introduced at the initial stage of modeling to enhance predictability and to avoid possible modeling biases. We propose a unified approach via penalized least squares. The penalty functions have to be singular at the origin to produce sparse solutions. Furthermore, the penalty function should be bounded by a constant to reduce bias.

\vspace{0.5em}
\noindent\textbf{KEY WORDS:} \ Hard thresholding; LASSO; Nonparametric curve; Penalized likelihood; Oracle estimator.
\end{abstractbox}


% 1. INTRODUCTION
\section{INTRODUCTION}

Variable selection is an important topic in linear regression analysis. In practice, a large number of predictors usually are introduced at the initial stage of modeling to enhance possible modeling biases. On the other hand, to enhance predictability and to avoid overfitting, statisticians usually use stepwise deletion and subset selection. The penalty functions have to be singular at the origin to produce sparse solutions. Furthermore, with proper choice of regularization parameters, we show that the proposed estimators perform as well as the oracle procedure in variable selection; namely, they work as well as if the correct submodel were known. Our simulation shows that the newly proposed penalized likelihood compare favorably with other variable selection techniques.

\subsection{Splines}

\subsection{Support Vector Reggresion}
\citet{vapnik1995nature} proposed the Support Vector Regression (SVR) framework by extending Support Vector Machine (SVM) to regression models. SVR constructs a regression function based on a subset of the data, known as support vectors. In its original form, Vapnik introduced the following representation:

\begin{equation}
F_2(x, \mathbf{w}) = \sum_{i=1}^{N} (\alpha_i^* - \alpha_i) (v_i^\top x + 1)^p + b
\end{equation}

Here, $\alpha_i$, $\alpha_i^*$ are Lagrange multipliers obtained from the dual optimization problem, $v_i$ represents the data samples, $p$ is the degree of the polynomial kernel, and the term $(v_i^\top x + 1)^p$ corresponds to the kernel function. This formula use a specific polinomial kernel.

The SVR model can be more generalized replacing the polynomial kernel with any positive-definite kernel function $K(x_i, x)$, leading to the more flexible representation:

\begin{equation}
f(x) = \sum_{i=1}^{N} (\alpha_i^* - \alpha_i) K(x_i, x) + b
\end{equation}

This form allows SVR to capture complex, non-linear relationships in data. Commonly used kernels include the radial basis function, linear and sigmoid kernels, making SVR a powerful and adaptable tool for regression tasks involving non-linear data structures.

\subsection{Loss Function and Hyperparameters}
The loss function utilized in this model is the $\varepsilon$-insensitive loss function, first introduced by \citet{vapnik1995nature}. This loss function defines a tube of tolerance around the regression function, within which errors are not penalized. The idea is to focus on fitting the data while ignoring small deviations that are likely due to noise (just like the svm do to separed classes). The loss function $L_\varepsilon$ is defined as:

\begin{equation}
L_\varepsilon(y, f(x)) = 
\begin{cases}
0, & \text{if } |y - f(x)| \leq \varepsilon \\
|y - f(x)| - \varepsilon, & \text{otherwise}
\end{cases}
\end{equation}

Only data points that lie outside the $\varepsilon$-tube contribute to the cost function, making the solution sparse in terms of support vectors \citep{smola1998tutorial}.

The Hyperparameters that are involve in this model are three and they are use to control the flexibility and generalization of the model:

\begin{itemize}
    \item \textbf{Regularization parameter $C$}: This controls the bias-variance trade-off of the model. A high $C$ encourages the model to minimize errors at the risk of overfitting, while a small $C$ yields a smoother function that may tolerate larger errors.
    \item \textbf{Epsilon $\varepsilon$}: Determines the width of the tube around the regression function where no penalty is applied. Larger values of $\varepsilon$ simplify the model by ignoring small deviations, while smaller values increase sensitivity to minor variations.
    \item \textbf{Kernel function $K(x_i, x_j)$}: Enables SVR to perform non-linear regression by implicitly mapping the input data into a higher-dimensional feature space. Common kernel choices include:
    \begin{itemize}
        \item \textit{Linear}: $K(x_i, x_j) = x_i^\top x_j$
        \item \textit{Polynomial}: $K(x_i, x_j) = (\gamma x_i^\top x_j + r)^d$
        \item \textit{Radial Basis Function (RBF)}: $K(x_i, x_j) = \exp(-\gamma \|x_i - x_j\|^2)$
        \item \textit{Sigmoid}: $K(x_i, x_j) = \tanh(\gamma x_i^\top x_j + r)$
    \end{itemize}
\end{itemize}

As noted by \citet{hastie2009elements}, the choice and tuning of these hyperparameters are crucial for model performance and are selected through cross-validation techniques.


\section{Simulation Study}

The penalized least squares idea can be extended naturally to likelihood-based models in various statistical contexts. Our approaches are distinguished from traditional methods (usually quadratic penalty) in that the penalty functions are symmetric, convex on $(0, \infty)$ (rather than concave for the negative quadratic penalty in the penalized likelihood situation), and possess singularities at the origin.
i




% Example of a mathematical equation
\begin{equation}
  \label{eq:example}
  p_{\lambda}(|\beta_j|) = \lambda^2 - (|\beta_j| - \lambda)^2 I(|\beta_j| < \lambda)
\end{equation}

% Example of an algorithm
\begin{algorithm}
  \caption{SCAD Algorithm}
  \label{alg:scad}
  \begin{algorithmic}[1]
    \State Initialize $\beta^{(0)} = $ least squares estimate
    \For{$k = 0, 1, 2, \ldots$ until convergence}
      \State Set weights $w_j = p'_{\lambda}(|\beta_j^{(k)}|)/|\beta_j^{(k)}|$
      \State Minimize $\|Y - X\beta\|^2 + \sum_{j=1}^p w_j \beta_j^2$
      \State Set $\beta^{(k+1)} = $ the minimizer
    \EndFor
  \end{algorithmic}
\end{algorithm}

% Example of a table
\begin{table}[ht]
  \centering
  \caption{Median Relative Model Error}
  \label{tab:example}
  \begin{tabular}{@{}lrrr@{}}
    \toprule
    Method & Model 1 & Model 2 & Model 3 \\
    \midrule
    SCAD & 0.35 & 0.42 & 0.38 \\
    LASSO & 0.60 & 0.58 & 0.59 \\
    Hard & 0.82 & 0.71 & 0.73 \\
    \bottomrule
  \end{tabular}
\end{table}

% 4. CONCLUSION
\section{CONCLUSION}

The penalized likelihood estimators perform as well as the oracle procedure in terms of selecting the correct submodel when the regularization parameter is appropriately chosen. In short, the proposed procedures outperform the maximum likelihood estimator and perform as well as the oracle. This is very analogous to the superefficiency phenomenon in the Hodges estimator.

\bibliography{citation}

\end{document}
